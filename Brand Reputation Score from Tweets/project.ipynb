{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BRAND REPUTATION CALCULATOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intructions\n",
    "The user will only have to use the last cell, although IPython Notebooks require to run the cells above for the declaration of the functions and the training of the system.\n",
    "\n",
    "The program works as follow, using the last cell:\n",
    "    - Introduce the name of the brand\n",
    "    - Choose between select several cities or a general reputation (there are examples of both)\n",
    "    - Run the cell and look at the results\n",
    "    \n",
    "The graphs are interactive so the user can zoom in, see the information staying over the bubble, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from collections import Counter\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "import ConfigParser\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import sys\n",
    "import re\n",
    "import io\n",
    "import os\n",
    "import ConfigParser\n",
    "from TwitterAPI import TwitterAPI\n",
    "from geopy.geocoders import Nominatim\n",
    "import math as m\n",
    "import plotly\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "import six\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "py.sign_in('jiranun1989','6eacuk3zt3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.auto_scroll_threshold = 9999;"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.auto_scroll_threshold = 9999;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweets search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_twitter(config_file):\n",
    "    config = ConfigParser.ConfigParser()\n",
    "    config.read(config_file)\n",
    "    twitter = TwitterAPI(\n",
    "                   config.get('twitter', 'consumer_key'),\n",
    "                   config.get('twitter', 'consumer_secret'),\n",
    "                   config.get('twitter', 'access_token'),\n",
    "                   config.get('twitter', 'access_token_secret'))\n",
    "    return twitter\n",
    "\n",
    "def request_by_location(twitter, r, numtweets):\n",
    "    \"\"\" If a Twitter request fails, sleep for 15 minutes.\n",
    "    Do this at most max_tries times before quitting.\n",
    "    Args:\n",
    "      twitter .... A TwitterAPI object.\n",
    "      city_name ... A list of cities to request\n",
    "      numtweets ..... How many tweets will be requested\n",
    "    Returns:\n",
    "      A dictionary with the name, the coordinates of the city and a list of tweets\n",
    "    \"\"\"\n",
    "    #The returned object\n",
    "    result={}\n",
    "    result['tweets']=[]\n",
    "    #Get location of the city\n",
    "    geolocator = Nominatim()\n",
    "    location = geolocator.geocode(r['city'], timeout=10)\n",
    "    result['location']=(location.latitude, location.longitude)\n",
    "    result['city']= r['city']\n",
    "    result['brand'] = r['brand']\n",
    "    #Conform the query parameters\n",
    "    params={}\n",
    "    params['q']= r['brand']\n",
    "    if r['city'] != 'general':\n",
    "        params['geocode']=''+str(location.latitude)+','+str(location.longitude)+',20mi'\n",
    "    params['lang'] = 'en'    \n",
    "    #if the require number is more than the maximum (only the first time)\n",
    "    nquery=numtweets\n",
    "    queriedtweets=0\n",
    "    max_page=100.0\n",
    "    last_id=0\n",
    "    if nquery > max_page:\n",
    "        nquery=max_page\n",
    "        \n",
    "\n",
    "    for i in range(int(m.ceil(numtweets/max_page))):\n",
    "        params['count']=nquery\n",
    "        if i==0:\n",
    "            request = twitter.request('search/tweets', params)\n",
    "        else:\n",
    "            params['since_id']=last_id\n",
    "            \n",
    "            request = twitter.request('search/tweets', params)\n",
    "        if request.status_code == 200:\n",
    "            queriedtweets += nquery\n",
    "            tweets=[]\n",
    "            for r in request:\n",
    "                tweets.append(r)\n",
    "                if float(r['id'])>last_id:\n",
    "                    last_id=float(r['id'])\n",
    "            result['tweets'].extend(tweets)\n",
    "            numtweets-=nquery\n",
    "        else:\n",
    "            print >> sys.stderr, 'Got error:', request.text, '\\nsleeping for 15 minutes.'\n",
    "            sys.stderr.flush()\n",
    "            time.sleep(60 * 15)\n",
    "        if numtweets>max_page:\n",
    "            nquery=max_page\n",
    "        else:\n",
    "            nquery=numtweets\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    text = re.sub('http\\S+', ' ', text)\n",
    "    text = re.sub('@\\S+', ' ', text)\n",
    "    text = text.lower()\n",
    "    text = text.replace('rt','')\n",
    "    return re.sub('\\W+', ' ', text).split()\n",
    "\n",
    "def get_training_files(path):\n",
    "    files = []\n",
    "    for (dirpath, dirnames, filenames) in os.walk(path):\n",
    "        for fn in filenames:\n",
    "            if fn.endswith(\".txt\"):\n",
    "                files.append(os.path.join(dirpath,fn))\n",
    "    return sorted(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_clf(c=1, penalty='l2'):\n",
    "    return LogisticRegression(random_state=42, C=c, penalty=penalty)\n",
    "\n",
    "def do_vec(texts):\n",
    "    global tokenize\n",
    "    vec = CountVectorizer(input='content',tokenizer=tokenize, min_df=2, max_df=.7, binary=True, ngram_range=(1,1))\n",
    "    X = vec.fit_transform(texts)\n",
    "    return X, vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Download the AFINN lexicon, unzip, and read the latest word list in AFINN-111.txt\n",
    "from StringIO import StringIO\n",
    "from zipfile import ZipFile\n",
    "from urllib import urlopen\n",
    "\n",
    "url = urlopen('http://www2.compute.dtu.dk/~faan/data/AFINN.zip')\n",
    "zipfile = ZipFile(StringIO(url.read()))\n",
    "afinn_file = zipfile.open('AFINN/AFINN-111.txt')\n",
    "\n",
    "afinn = dict()\n",
    "\n",
    "for line in afinn_file:\n",
    "    parts = line.strip().split()\n",
    "    if len(parts) == 2:\n",
    "        afinn[parts[0]] = int(parts[1])\n",
    "\n",
    "def afinn_sentiment(terms, afinn, verbose=False):\n",
    "    pos = 0\n",
    "    neg = 0\n",
    "    for t in terms:\n",
    "        if t in afinn:\n",
    "            if verbose:\n",
    "                print '\\t%s=%d' % (t, afinn[t])\n",
    "            if afinn[t] > 0:\n",
    "                pos += afinn[t]\n",
    "            else:\n",
    "                neg += -1 * afinn[t]\n",
    "    return (pos, neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Get the prediction using AFINN method\n",
    "def get_AFINN_prediction(texts):\n",
    "    res = []\n",
    "    for i in range(len(texts)):\n",
    "        terms = tokenize(texts[i])\n",
    "        afinn_score = afinn_sentiment(terms, afinn)\n",
    "        norm_afinn_score = 0.\n",
    "        if afinn_score[0]+afinn_score[1] != 0:\n",
    "            norm_afinn_score = float(afinn_score[0]-afinn_score[1])/float(afinn_score[0]+afinn_score[1])\n",
    "        res.append(norm_afinn_score)\n",
    "    return np.array(res)\n",
    "\n",
    "#Get the sentiment prediction for the tweet\n",
    "def get_prediction(texts):\n",
    "    nTweets = len(texts)\n",
    "    X = vec.transform(texts)\n",
    "    clf_predicts = clf.predict(X)\n",
    "    AFINN_predicts = get_AFINN_prediction(texts)\n",
    "    \n",
    "    avg_predicts = [(float(clf_predicts[i])+float(AFINN_predicts[i]))/2. for i in range(nTweets)]\n",
    "    return np.array(avg_predicts) \n",
    "\n",
    "#Get the most common hashtags used\n",
    "def get_popular_hashtag(texts):\n",
    "    c = Counter()\n",
    "    pat = re.compile(r\"#(\\w+)\")\n",
    "    for t in texts:\n",
    "        hasgtags = pat.findall(t)\n",
    "        c.update(hasgtags)\n",
    "    return c.most_common(10)\n",
    "\n",
    "#Obtain the score based on popularity\n",
    "def get_popular_score(newest_tweet, oldest_tweet, nTweets):\n",
    "\n",
    "    ts1 = time.mktime(time.strptime(oldest_tweet,'%a %b %d %H:%M:%S +0000 %Y'))\n",
    "    ts2 = time.mktime(time.strptime(newest_tweet,'%a %b %d %H:%M:%S +0000 %Y'))\n",
    "    diff = ts2-ts1\n",
    "    rate = diff/nTweets #seconds per tweet\n",
    "    \n",
    "    # less than 10 seconds per tweet\n",
    "    if rate <= 10. : \n",
    "        score = 1.\n",
    "    # less than 30 seconds per tweet\n",
    "    elif rate <= 30.:\n",
    "        score = .95\n",
    "    # less than 1 minute per tweet\n",
    "    elif rate <= 60.:\n",
    "        score = .9\n",
    "    # less than 10 minutes per tweet\n",
    "    elif rate <= 600 :\n",
    "        score = .85\n",
    "    # less than 1 hour per tweet\n",
    "    elif rate <= 3600 :\n",
    "        score = .75\n",
    "    # less than 1 day per tweet\n",
    "    elif rate <= 3600*24 :\n",
    "        score = .7\n",
    "    # less than 1 week per tweet\n",
    "    elif rate <= 3600*24*7 :\n",
    "        score = .65\n",
    "    # less than 1 month per tweet\n",
    "    elif rate <= 3600*24*30 :\n",
    "        score = .55\n",
    "    else:\n",
    "        score = .2\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Compute the sentiment of the tweets given\n",
    "def get_reputation_score(info):\n",
    "    location = info['location']\n",
    "    city = info['city']\n",
    "    tweets = info['tweets']\n",
    "    \n",
    "    tweet_texts = [t['text'] for t in tweets]\n",
    "    avg_predicts = get_prediction(tweet_texts)\n",
    "    \n",
    "    tweet_nRT = [t['retweet_count'] for t in tweets]\n",
    "    tweet_likes = [t['favorite_count'] for t in tweets]\n",
    "    users = [t['user'] for t in tweets]\n",
    "    created_times = [t['created_at'] for t in tweets]\n",
    "    nTweets = len(tweet_texts)\n",
    "    sum_weight = 0.\n",
    "\n",
    "    for i in range(nTweets):\n",
    "        rt_count = tweet_nRT[i]\n",
    "        like_count = tweet_likes[i]\n",
    "        follower_count = users[i]['followers_count']\n",
    "        mult = 1.\n",
    "        \n",
    "        if rt_count > 50 and rt_count <= 200 :\n",
    "            mult *= 3.\n",
    "        elif rt_count > 200 and rt_count <= 1000 :\n",
    "            mult *= 5.\n",
    "        elif rt_count > 1000 :\n",
    "            mult *= 10.\n",
    "        \n",
    "        if like_count > 50 and like_count <= 200 :\n",
    "            mult *= 2.\n",
    "        elif like_count > 200 and like_count <= 1000 :\n",
    "            mult *= 3.\n",
    "        elif like_count > 1000 :\n",
    "            mult *= 4.\n",
    "            \n",
    "        if follower_count > 500 and follower_count <= 5000 :\n",
    "            mult *= 3.\n",
    "        elif follower_count > 5000 and follower_count <= 50000 :\n",
    "            mult *= 5.\n",
    "        elif follower_count > 50000:\n",
    "            mult *= 10\n",
    "        \n",
    "        sum_weight += mult\n",
    "        avg_predicts[i] = avg_predicts[i]*mult\n",
    "\n",
    "    senti_score = sum(avg_predicts)/sum_weight\n",
    "\n",
    "    # Normalize to [0,1]\n",
    "    senti_score = (senti_score + 1)/2.\n",
    "    \n",
    "    if nTweets > 1:\n",
    "        pop_score = get_popular_score(created_times[0], created_times[nTweets-1], nTweets)\n",
    "    else :\n",
    "        pop_score = 0.\n",
    "\n",
    "        \n",
    "    # Ratio between sentiment score : popular score = 1:2\n",
    "    score = (1.*senti_score + 2.* pop_score)/3.\n",
    "    \n",
    "    return score*10000, get_popular_hashtag(tweet_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 8 files\n",
      "504 tweets have been read\n",
      "-  251  negative tweets\n",
      "-  253  positive tweets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr',\n",
       "          penalty='l2', random_state=42, solver='liblinear', tol=0.0001,\n",
       "          verbose=0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training the system\n",
    "files = get_training_files('data')\n",
    "\n",
    "training_tweets = []\n",
    "for fname in files:\n",
    "    f = open(fname, 'r')\n",
    "    for line in f:\n",
    "        toks = line.lower().rstrip('\\n').split('\\t')\n",
    "        training_tweets.append(toks)\n",
    "print 'From',len(files),'files'\n",
    "print len(training_tweets),'tweets have been read'\n",
    "print '- ',len([t for t in training_tweets if t[0]=='-1']),' negative tweets'\n",
    "print '- ',len([t for t in training_tweets if t[0]=='1']),' positive tweets'\n",
    "\n",
    "train_texts = np.array([tweet[2].replace(tweet[1],'') for tweet in training_tweets])\n",
    "labels = np.array([tweet[0] for tweet in training_tweets])\n",
    "\n",
    "clf = get_clf(c=1.)\n",
    "X, vec = do_vec(train_texts)\n",
    "clf.fit(X, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scores display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Show the chart of most used hashtags\n",
    "def get_words_graph(result):\n",
    "    words=result['hashtags']\n",
    "    wx=[]\n",
    "    wy=[]\n",
    "    wtext=[]\n",
    "    wsize=[]\n",
    "    count=3\n",
    "    words = sorted(words, key=lambda w: w[1])\n",
    "    for w in words:\n",
    "        wy.append(w[1])\n",
    "        wx.append(count)\n",
    "        count+=5\n",
    "        wtext.append(w[0])\n",
    "        wsize.append(int(w[1])*5)\n",
    "        \n",
    "    trace = go.Scatter(\n",
    "        x=wx,\n",
    "        y=wy,\n",
    "        mode='markers+text',\n",
    "        name='Most used hashtags',\n",
    "        text=wtext,\n",
    "        marker=dict(\n",
    "            sizemode='diameter',\n",
    "            sizeref=0.85,\n",
    "            size=wsize,\n",
    "            line=dict(\n",
    "                width=2\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    "    data = go.Data([trace])\n",
    "    layout = go.Layout(\n",
    "        title='Most used hashtags',\n",
    "        xaxis=dict(\n",
    "            gridcolor='rgb(255, 255, 255)',\n",
    "            range=[0, 60],\n",
    "            type='linear',\n",
    "            zerolinewidth=1,\n",
    "            ticklen=5,\n",
    "            gridwidth=2,\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            title='Number of tweets',\n",
    "            gridcolor='rgb(255, 255, 255)',\n",
    "            range=[-5, 50],\n",
    "            zerolinewidth=1,\n",
    "            ticklen=5,\n",
    "            gridwidth=2,\n",
    "        ),\n",
    "        paper_bgcolor='rgb(243, 243, 243)',\n",
    "        plot_bgcolor='rgb(243, 243, 243)',\n",
    "    )\n",
    "    fig = go.Figure(data=data, layout=layout)\n",
    "    display(py.iplot(fig, filename='most-used-hashtags'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Internal used functions:\n",
    "def get_color(points, limits, colors):\n",
    "    for i in range(len(limits)):\n",
    "        if points > limits[i][0] and points < limits[i][1]:\n",
    "            return colors[i]\n",
    "\n",
    "def get_range(points, limits):\n",
    "    for i in range(len(limits)):\n",
    "        if points > limits[i][0] and points < limits[i][1]:\n",
    "            return limits[i]\n",
    "\n",
    "#Show cities reputation\n",
    "def show_map_reputation(results):\n",
    "\n",
    "    limits = [(0.0,1900.0),(2000.0,3900.0),(4000.0,5900.0),(6000.0,7900.0),(8000.0,10000.0)]\n",
    "    colors = [\"rgb(255,0,0)\",\"rgb(255,55,0)\",\"rgb(255,255,75)\",\"rgb(55,255,0)\",\"rgb(0,255,0)\"]\n",
    "    cities = []\n",
    "    scale = 50\n",
    "\n",
    "    cities=[]\n",
    "    cities.append(go.Scattergeo(\n",
    "        lat=[c['location'][0] for c in results],\n",
    "        lon=[c['location'][1] for c in results],\n",
    "        marker={\"color\": [get_color(c['reputation'], limits, colors) for c in results],\n",
    "                \"line\": {\"width\": 1},\n",
    "                \"size\": 30\n",
    "            },\n",
    "        mode=\"markers\",\n",
    "        name= \"\",\n",
    "        text=[c['city'].title()+'<br>Reputation:'+str(c['reputation']) for c in results],\n",
    "#         textposition = \"bottom center\"\n",
    "        ))\n",
    "        \n",
    "        \n",
    "    layout = dict(\n",
    "            title = 'Reputation by city',\n",
    "            showlegend = False,\n",
    "            geo = dict(\n",
    "                scope='usa',\n",
    "                showland = True,\n",
    "                landcolor = 'rgb(217, 217, 217)',\n",
    "                subunitwidth=1,\n",
    "                countrywidth=1,\n",
    "                subunitcolor=\"rgb(255, 255, 255)\",\n",
    "                countrycolor=\"rgb(255, 255, 255)\"\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    fig = dict( data=cities, layout=layout )\n",
    "    display(py.iplot( fig, validate=False, filename='reputation-by-city'))\n",
    "\n",
    "#Show the reputation points\n",
    "def show_points(result):\n",
    "    points = result['reputation']\n",
    "    source = \"\"\"\n",
    "    <h1>Reputation score for \"\"\"+result['brand']\n",
    "    if result['city']!= 'general':\n",
    "        source+=\"\"\" in \"\"\"+result['city'].title()\n",
    "    \n",
    "    source +=\"\"\":</h1>\n",
    "    <h2>\"\"\"+\"%.2f\" % result['reputation'] +\"\"\"/10000 points</h2>\n",
    "    \"\"\"\n",
    "    display(HTML(source))\n",
    "\n",
    "\n",
    "#Show all the results for a query\n",
    "def show_results(results):\n",
    "    if len(results) ==1:\n",
    "        show_points(results[0])\n",
    "        get_words_graph(results[0])\n",
    "    else:\n",
    "        for r in results:\n",
    "            show_points(r)\n",
    "            get_words_graph(r)\n",
    "        show_map_reputation(results)\n",
    "        \n",
    "def get_reputation(brand, cities):\n",
    "    twitter = get_twitter('twitter.cfg')\n",
    "    results=[]\n",
    "    numtweets=100\n",
    "    for c in cities:\n",
    "        r={'brand': brand, 'city': c}\n",
    "        r = request_by_location(twitter, r, numtweets)\n",
    "        results.append(r)\n",
    "        r['reputation'], r['hashtags'] = get_reputation_score(r)\n",
    "    show_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Reputation request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <h1>Reputation score for Apple in San Francisco:</h1>\n",
       "    <h2>6655.17/10000 points</h2>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\"seamless=\"seamless\" src=\"https://plot.ly/~Jiranun1989/3.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <h1>Reputation score for Apple in New York:</h1>\n",
       "    <h2>6763.50/10000 points</h2>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\"seamless=\"seamless\" src=\"https://plot.ly/~Jiranun1989/3.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\"seamless=\"seamless\" src=\"https://plot.ly/~Jiranun1989/5.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "brand_name = 'Apple'\n",
    "#General or list of cities\n",
    "cities = ['san francisco', 'new york']\n",
    "#cities = ['general']\n",
    "get_reputation(brand_name, cities)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

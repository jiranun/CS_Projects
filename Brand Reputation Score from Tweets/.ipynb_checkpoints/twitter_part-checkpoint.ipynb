{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation for score calculation\n",
    "1. request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "import ConfigParser\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import sys\n",
    "import time\n",
    "import re\n",
    "import io\n",
    "import os\n",
    "import numpy as np\n",
    "from TwitterAPI import TwitterAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    text = re.sub('http\\S+', ' ', text)\n",
    "    text = re.sub('@\\S+', ' ', text)\n",
    "    text = text.lower()\n",
    "    text = text.replace('rt','')\n",
    "    return re.sub('\\W+', ' ', text).split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_training_files(path):\n",
    "    \"\"\" Return a list of file names in this directory that end in .txt \n",
    "    The list should be sorted alphabetically by file name.\n",
    "    Params:\n",
    "        path....a directory containing .txt review files.\n",
    "    Returns:\n",
    "        a list of .txt file names, sorted alphabetically.\n",
    "    \"\"\"\n",
    "    ###TODO\n",
    "    files = []\n",
    "    for (dirpath, dirnames, filenames) in os.walk(path):\n",
    "        for fn in filenames:\n",
    "            if fn.endswith(\".txt\"):\n",
    "                files.append(os.path.join(dirpath,fn))\n",
    "    return sorted(files)\n",
    "    ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 8 files\n",
      "504 tweets have been read\n",
      "-  251  negative tweets\n",
      "-  253  positive tweets\n"
     ]
    }
   ],
   "source": [
    "files = get_training_files('data')\n",
    "\n",
    "training_tweets = []\n",
    "for fname in files:\n",
    "    f = open(fname, 'r')\n",
    "    for line in f:\n",
    "        toks = line.lower().rstrip('\\n').split('\\t')\n",
    "        training_tweets.append(toks)\n",
    "print 'From',len(files),'files'\n",
    "print len(training_tweets),'tweets have been read'\n",
    "print '- ',len([t for t in training_tweets if t[0]=='-1']),' negative tweets'\n",
    "print '- ',len([t for t in training_tweets if t[0]=='1']),' positive tweets'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_texts = np.array([tweet[2].replace(tweet[1],'') for tweet in training_tweets])\n",
    "labels = np.array([tweet[0] for tweet in training_tweets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_clf(c=1, penalty='l2'):\n",
    "    return LogisticRegression(random_state=42, C=c, penalty=penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def do_vec(texts):\n",
    "    global tokenize\n",
    "    vec = CountVectorizer(input='content',tokenizer=tokenize, min_df=2, max_df=.7, binary=True, ngram_range=(1,1))\n",
    "    X = vec.fit_transform(texts)\n",
    "    return X, vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when c = 1e-05 , average cross validation accuracy= 0.650871287129\n",
      "when c = 0.0001 , average cross validation accuracy= 0.650871287129\n",
      "when c = 0.001 , average cross validation accuracy= 0.650871287129\n",
      "when c = 0.01 , average cross validation accuracy= 0.682574257426\n",
      "when c = 0.09 , average cross validation accuracy= 0.722178217822\n",
      "when c = 0.1 , average cross validation accuracy= 0.724178217822\n",
      "when c = 0.5 , average cross validation accuracy= 0.74798019802\n",
      "when c = 1 , average cross validation accuracy= 0.74596039604\n",
      "when c = 5 , average cross validation accuracy= 0.74203960396\n",
      "when c = 10 , average cross validation accuracy= 0.732099009901\n",
      "when c = 1000 , average cross validation accuracy= 0.684475247525\n"
     ]
    }
   ],
   "source": [
    "def do_cross_validation(X, y, n_folds=5, verbose=False, c=1):\n",
    "    \"\"\"\n",
    "    Perform n-fold cross validation, calling get_clf() to train n\n",
    "    different classifiers. Use sklearn's KFold class: http://goo.gl/wmyFhi\n",
    "    Be sure not to shuffle the data, otherwise your output will differ.\n",
    "    Params:\n",
    "        X.........a csr_matrix of feature vectors\n",
    "        y.........the true labels of each document\n",
    "        n_folds...the number of folds of cross-validation to do\n",
    "        verbose...If true, report the testing accuracy for each fold.\n",
    "    Return:\n",
    "        the average testing accuracy across all folds.\n",
    "    \"\"\"\n",
    "    ###TODO\n",
    "    kf = KFold(len(y), n_folds=n_folds)\n",
    "    fold_number = 0\n",
    "    accuracies = []\n",
    "    for train_index, test_index in kf:\n",
    "        clf = get_clf(c=c)\n",
    "        clf.fit(X[train_index], y[train_index])\n",
    "        predicted = clf.predict(X[test_index])\n",
    "        acc = accuracy_score(y[test_index], predicted)\n",
    "        accuracies.append(acc)\n",
    "        if verbose:\n",
    "            print \"fold \"+str(fold_number)+\" accuracy=\"+str(acc)\n",
    "        fold_number += 1\n",
    "    return np.mean(accuracies)\n",
    "    ###\n",
    "    \n",
    "X, vec = do_vec(train_texts)\n",
    "cs = [.00001,.0001,.001,.01,.09, .1,.5, 1, 5, 10, 1000]\n",
    "# cs = [1]\n",
    "for c in cs:\n",
    "    print 'when c =',c,', average cross validation accuracy=',do_cross_validation(X, labels, verbose=False, c=c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Established Twitter connection.\n"
     ]
    }
   ],
   "source": [
    "def get_twitter(config_file):\n",
    "    config = ConfigParser.ConfigParser()\n",
    "    config.read(config_file)\n",
    "    twitter = TwitterAPI(\n",
    "                   config.get('twitter', 'consumer_key'),\n",
    "                   config.get('twitter', 'consumer_secret'),\n",
    "                   config.get('twitter', 'access_token'),\n",
    "                   config.get('twitter', 'access_token_secret'))\n",
    "    return twitter\n",
    "\n",
    "twitter = get_twitter('twitter_wii.cfg')\n",
    "print('Established Twitter connection.')\n",
    "\n",
    "def robust_request(twitter, resource, params, max_tries=5):\n",
    "    for i in range(max_tries):\n",
    "        request = twitter.request(resource, params)\n",
    "        if request.status_code == 200:\n",
    "            return request\n",
    "        else:\n",
    "            print >> sys.stderr, 'Got error:', request.text, '\\nsleeping for 15 minutes.'\n",
    "            sys.stderr.flush()\n",
    "            time.sleep(61 * 15)\n",
    "\n",
    "def get_info(bn):\n",
    "    request = robust_request(twitter, 'search/tweets', {'q': bn, 'count':100, 'lang':'en'})\n",
    "    return {'location':1111,'city':'abc','tweets':request}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Download the AFINN lexicon, unzip, and read the latest word list in AFINN-111.txt\n",
    "from StringIO import StringIO\n",
    "from zipfile import ZipFile\n",
    "from urllib import urlopen\n",
    "\n",
    "url = urlopen('http://www2.compute.dtu.dk/~faan/data/AFINN.zip')\n",
    "zipfile = ZipFile(StringIO(url.read()))\n",
    "afinn_file = zipfile.open('AFINN/AFINN-111.txt')\n",
    "\n",
    "afinn = dict()\n",
    "\n",
    "for line in afinn_file:\n",
    "    parts = line.strip().split()\n",
    "    if len(parts) == 2:\n",
    "        afinn[parts[0]] = int(parts[1])\n",
    "\n",
    "def afinn_sentiment(terms, afinn, verbose=False):\n",
    "    pos = 0\n",
    "    neg = 0\n",
    "    for t in terms:\n",
    "        if t in afinn:\n",
    "            if verbose:\n",
    "                print '\\t%s=%d' % (t, afinn[t])\n",
    "            if afinn[t] > 0:\n",
    "                pos += afinn[t]\n",
    "            else:\n",
    "                neg += -1 * afinn[t]\n",
    "    return (pos, neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr',\n",
       "          penalty='l2', random_state=42, solver='liblinear', tol=0.0001,\n",
       "          verbose=0)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = get_clf(c=1.)\n",
    "X, vec = do_vec(train_texts)\n",
    "clf.fit(X, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_AFINN_prediction(texts):\n",
    "    res = []\n",
    "    for i in range(len(texts)):\n",
    "        terms = tokenize(texts[i])\n",
    "        afinn_score = afinn_sentiment(terms, afinn)\n",
    "        norm_afinn_score = 0.\n",
    "        if afinn_score[0]+afinn_score[1] != 0:\n",
    "            norm_afinn_score = float(afinn_score[0]-afinn_score[1])/float(afinn_score[0]+afinn_score[1])\n",
    "        res.append(norm_afinn_score)\n",
    "    return np.array(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for AFINN is 0.769841269841\n"
     ]
    }
   ],
   "source": [
    "results = get_AFINN_prediction(train_texts)\n",
    "results = np.array(['1' if i>0. else '-1' for i in results])\n",
    "\n",
    "print 'Accuracy score for AFINN is', accuracy_score(labels,results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_prediction(texts):\n",
    "    nTweets = len(texts)\n",
    "    X = vec.transform(texts)\n",
    "    clf_predicts = clf.predict(X)\n",
    "    AFINN_predicts = get_AFINN_prediction(texts)\n",
    "    \n",
    "    avg_predicts = [(float(clf_predicts[i])+float(AFINN_predicts[i]))/2. for i in range(nTweets)]\n",
    "    return np.array(avg_predicts) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_popular_words(texts):\n",
    "    c = Counter()\n",
    "    for t in texts:\n",
    "        words = tokenize(t)\n",
    "        c.update(words)\n",
    "    return c.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_popular_hashtag(texts):\n",
    "    c = Counter()\n",
    "    pat = re.compile(r\"#(\\w+)\")\n",
    "    for t in texts:\n",
    "        hasgtags = pat.findall(t)\n",
    "        c.update(hasgtags)\n",
    "    return c.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_popular_score(newest_tweet, oldest_tweet, nTweets):\n",
    "    ts1 = time.mktime(time.strptime(oldest_tweet,'%a %b %d %H:%M:%S +0000 %Y'))\n",
    "    ts2 = time.mktime(time.strptime(newest_tweet,'%a %b %d %H:%M:%S +0000 %Y'))\n",
    "    diff = ts2-ts1\n",
    "    rate = diff/nTweets #seconds per tweet\n",
    "\n",
    "    # less than 10 seconds per tweet\n",
    "    if rate <= 10. : \n",
    "        score = 1.\n",
    "    # less than 30 seconds per tweet\n",
    "    elif rate <= 30.:\n",
    "        score = .95\n",
    "    # less than 1 minute per tweet\n",
    "    elif rate <= 60.:\n",
    "        score = .9\n",
    "    # less than 10 minutes per tweet\n",
    "    elif rate <= 600 :\n",
    "        score = .85\n",
    "    # less than 1 hour per tweet\n",
    "    elif rate <= 3600 :\n",
    "        score = .75\n",
    "    # less than 1 day per tweet\n",
    "    elif rate <= 3600*24 :\n",
    "        score = .7\n",
    "    # less than 1 week per tweet\n",
    "    elif rate <= 3600*24*7 :\n",
    "        score = .65\n",
    "    # less than 1 month per tweet\n",
    "    elif rate <= 3600*24*30 :\n",
    "        score = .55\n",
    "    else:\n",
    "        score = .2\n",
    "        \n",
    "    if nTweets < 100:\n",
    "        score *= .5\n",
    "    \n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_reputation_score(info):\n",
    "    location = info['location']\n",
    "    city = info['city']\n",
    "    tweets = info['tweets']\n",
    "    \n",
    "    tweet_texts = [t['text'] for t in tweets]\n",
    "    avg_predicts = get_prediction(tweet_texts)\n",
    "    \n",
    "    tweet_nRT = [t['retweet_count'] for t in tweets]\n",
    "    tweet_likes = [t['favorite_count'] for t in tweets]\n",
    "    users = [t['user'] for t in tweets]\n",
    "    created_times = [t['created_at'] for t in tweets]\n",
    "    nTweets = len(tweet_texts)\n",
    "    sum_weight = 0.\n",
    "\n",
    "    for i in range(nTweets):\n",
    "        rt_count = tweet_nRT[i]\n",
    "        like_count = tweet_likes[i]\n",
    "        follower_count = users[i]['followers_count']\n",
    "        mult = 1.\n",
    "        \n",
    "        if rt_count > 50 and rt_count <= 200 :\n",
    "            mult *= 3.\n",
    "        elif rt_count > 200 and rt_count <= 1000 :\n",
    "            mult *= 5.\n",
    "        elif rt_count > 1000 :\n",
    "            mult *= 10.\n",
    "        \n",
    "        if like_count > 50 and like_count <= 200 :\n",
    "            mult *= 2.\n",
    "        elif like_count > 200 and like_count <= 1000 :\n",
    "            mult *= 3.\n",
    "        elif like_count > 1000 :\n",
    "            mult *= 4.\n",
    "            \n",
    "        if follower_count > 500 and follower_count <= 5000 :\n",
    "            mult *= 3.\n",
    "        elif follower_count > 5000 and follower_count <= 50000 :\n",
    "            mult *= 5.\n",
    "        elif follower_count > 50000:\n",
    "            mult *= 10\n",
    "        \n",
    "        sum_weight += mult\n",
    "        avg_predicts[i] = avg_predicts[i]*mult\n",
    "\n",
    "    senti_score = sum(avg_predicts)/sum_weight\n",
    "\n",
    "    # Normalize to [0,1]\n",
    "    senti_score = (senti_score + 1)/2.\n",
    "    \n",
    "    if nTweets > 1:\n",
    "        pop_score = get_popular_score(created_times[0], created_times[nTweets-1], nTweets)\n",
    "    else :\n",
    "        pop_score = 0.\n",
    "    \n",
    "    # Ratio between sentiment score : popular score = 1:2\n",
    "    score = (1.*senti_score + 2.* pop_score)/3.\n",
    "    \n",
    "    return score, get_popular_hashtag(tweet_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "google got 0.873938158342\n",
      "apple got 0.824780028647\n",
      "ikea got 0.859856237817\n",
      "fedex got 0.752795841101\n",
      "Heineken got 0.805586305863\n",
      "Toshiba got 0.825823045267\n",
      "Carlsberg got 0.798347701149\n",
      "usps got 0.723977253121\n"
     ]
    }
   ],
   "source": [
    "brandnames = ['google', 'apple', 'ikea', 'fedex','Heineken', 'Toshiba', 'Carlsberg','usps']\n",
    "\n",
    "for b in brandnames:\n",
    "    inf = get_info(b)\n",
    "    score, hashtags = get_reputation_score(inf)\n",
    "    print b,\"got\",str(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# brandnames = ['usps', 'ups', 'dhl']\n",
    "# f = open('traning_data8.txt','w')\n",
    "# for b in brandnames:\n",
    "#     request = robust_request(twitter, 'search/tweets', {'q': b, 'count':100, 'lang':'en'})\n",
    "#     for tweet in request:\n",
    "#         text = b+'\\t'+tweet['text']+'\\n'\n",
    "#         text = text.encode('utf8')\n",
    "#         f.write(str(text))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
